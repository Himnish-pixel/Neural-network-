Neural Network Digit Recognizer (NumPy Only)
ğŸ“Œ Overview

This project implements a fully connected neural network from scratch, using only NumPy, to classify handwritten digits from the MNIST Digit Recognizer Dataset.
The goal is to demonstrate how forward propagation, backpropagation, gradient descent, activation functions, and loss computation work internallyâ€”without relying on deep-learning libraries like TensorFlow or PyTorch.

âœ¨ Features

âœ… Built completely from scratch using NumPy
âœ… Works on the Kaggle Digit Recognizer dataset
âœ… Supports 2 hidden layers
âœ… Uses ReLU activation for hidden layers
âœ… Uses Softmax for output
âœ… Uses Cross-Entropy Loss
âœ… Trains using batch gradient descent
âœ… Outputs loss every 100 epochs
âœ… Easy-to-understand code structure

ğŸ§  Neural Network Architecture

The network uses the following structure:

Layer	Type	Size
Input Layer	Flattened pixels	784 neurons
Hidden Layer 1	Dense + ReLU	64 neurons
Hidden Layer 2	Dense + ReLU	32 neurons
Output Layer	Dense + Softmax	10 neurons (digits 0-9)
ğŸ“‚ Dataset

You must download the Kaggle MNIST dataset:

train.csv â€” contains 42k images + labels

test.csv â€” contains images (for final evaluation)

Each image is 28Ã—28 pixels flattened into 784 columns.

ğŸš€ Training Workflow

Load dataset using Pandas

Normalize pixel values to [0,1]

Convert labels into one-hot encoded vectors

Initialize weights & biases

Perform:

Forward propagation

Loss calculation

Backpropagation

Gradient descent update

Repeat for the given number of epochs

ğŸ› ï¸ Technologies Used

NumPy â†’ Total math backend

Pandas â†’ Data loading

Matplotlib (optional) â†’ Visualization

No ML libraries are usedâ€”everything is handcrafted.

ğŸ“ˆ Example Training Output
Epoch 0   | Loss: 2.3015
Epoch 100 | Loss: 0.7421
Epoch 200 | Loss: 0.4238
Epoch 300 | Loss: 0.2890
...

ğŸ“‘ How to Run
pip install numpy pandas matplotlib
python train_digit_recognizer.py


Make sure your dataset path (train.csv) is correct.

ğŸ“Œ Future Improvements

Add mini-batch training

Add Adam optimizer

Add Dropout

Evaluate accuracy on test set

Save & load model weights
